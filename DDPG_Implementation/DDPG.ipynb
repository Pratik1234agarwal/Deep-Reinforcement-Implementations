{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    state,reward,done,_ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n",
      "Box(3,)\n",
      "State size :  3  Action Size:  1\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "print(\"State size : \",state_size,\" Action Size: \",action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(nn.Linear(input_size,16),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(16,output_size),\n",
    "                                  nn.Tanh())\n",
    "    def forward(self,state):\n",
    "        ac = self.actor(state)\n",
    "        return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.fcs1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32+action_size, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self,state,action):\n",
    "        \n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,input_size,output_size):\n",
    "        \n",
    "        \n",
    "        self.buffer_size = int(1e6)\n",
    "        self.actor_lr = 1e-4\n",
    "        self.critic_lr = 1e-3\n",
    "        self.tau = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.actor_local = Actor(input_size,output_size)\n",
    "        self.actor_target = Actor(input_size,output_size)\n",
    "        \n",
    "        self.critic_local = Critic(input_size)\n",
    "        self.critic_target = Critic(input_size)\n",
    "        \n",
    "        self.memory = deque(maxlen=self.buffer_size)\n",
    "        self.noise = OUNoise(action_size)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(),lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(),lr=self.critic_lr)\n",
    "        \n",
    "        self.ac_loss = []\n",
    "        self.cr_loss = []\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "    def act(self,state,add_noise=True):\n",
    "        #state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        state = torch.from_numpy(state).float()\n",
    "        self.actor_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).detach().numpy()\n",
    "        if add_noise:\n",
    "            action+=self.noise.sample()\n",
    "        \n",
    "        return np.clip(action,-1,1)\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "    \n",
    "    def step(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.train()\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.int8)).float().to(device)\n",
    "        \n",
    "        return states,actions,rewards,next_states,dones\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        states,actions,rewards,next_states,dones = self.sample()\n",
    "        \n",
    "        action_target = self.actor_target(next_states)\n",
    "        y = rewards + self.gamma*self.critic_target(next_states,action_target)*(1-dones)\n",
    "        \n",
    "        Q_expected = self.critic_local(states,actions)\n",
    "        critic_loss = F.mse_loss(Q_expected,y)\n",
    "        self.cr_loss.append(critic_loss.item())\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states,actions_pred).mean()\n",
    "        self.ac_loss.append(actor_loss.item())\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.actor_target,self.actor_local,self.tau)\n",
    "        self.soft_update(self.critic_target,self.critic_local,self.tau)\n",
    "    def soft_update(self,target_model,local_model,tau):\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the DDPG Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size,action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1625  , Score :  -1106.7200061827898     Avg. of last 100 : -1087.173743"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 5000\n",
    "reward_per_ep = []\n",
    "avg_last_100 = []\n",
    "for ep in range(1,n_episodes+1):\n",
    "    state = env.reset()\n",
    "    state = state.reshape(1,3)\n",
    "    score = 0\n",
    "    agent.reset()\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        score+=reward[0]\n",
    "        next_state = next_state.reshape(1,3)\n",
    "        agent.step(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    reward_per_ep.append(score)\n",
    "    print(\"\\rEpisode \",ep,\" , Score : \",score,\"  \",end=\"\")\n",
    "    if ep > 100 :\n",
    "        avg = np.mean(reward_per_ep[-100:])\n",
    "        avg_last_100.append(avg)\n",
    "        print(\"  Avg. of last 100 : {:3f}\".format(avg),end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(reward_per_ep))+1\n",
    "plt.plot(x,reward_per_ep,label='Scores')\n",
    "#plt.plot(x+64,agent.ac_loss,'Actor Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(avg_last_100))+100\n",
    "plt.plot(x,avg_last_100,label='Avg. Scores')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.ac_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
