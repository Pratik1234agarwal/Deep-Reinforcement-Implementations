{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pendulum-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    def _thunk():\n",
    "        env=gym.make(env_name)\n",
    "        return env\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x-=x.mean()\n",
    "    x /= (x.std()+1e-8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size  3  State info :  Box(3,)\n",
      "Action size  1  Action info :  Box(1,)\n"
     ]
    }
   ],
   "source": [
    "num_inputs = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.shape[0]\n",
    "print(\"State size \",num_inputs,\" State info : \",env.observation_space)\n",
    "\n",
    "print(\"Action size \",num_outputs,\" Action info : \",env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(x):\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,input_size,output_size,std=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(nn.Linear(input_size,32),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(32,16) ,\n",
    "                                  nn.Linear(16,output_size))\n",
    "        self.critic = nn.Sequential(nn.Linear(input_size,32),\n",
    "                                    nn.Linear(32,16),\n",
    "                                    nn.Linear(16,1))\n",
    "        self.log_std = nn.Parameter(torch.ones(1,output_size)*std)\n",
    "    def forward(self,state):\n",
    "        value = self.critic(state)\n",
    "        mu = self.actor(state)\n",
    "        std = self.log_std.exp()\n",
    "        dist = Normal(mu,std)\n",
    "        return dist,value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_env = 8 \n",
    "gamma = 0.9\n",
    "lr = 1e-4\n",
    "gae_lambda = 0.95\n",
    "epsilon = 0.2\n",
    "entropy_beta = 0.001\n",
    "critic_discount = 0.5\n",
    "PPO_STEPS = 256\n",
    "mini_batch = 64\n",
    "ppo_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value,rewards,masks,values,gamma=gamma,lam=gae_lambda):\n",
    "    values = values+[next_value]\n",
    "    gae = 0 \n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma*values[step+1]*masks[step] - values[step]\n",
    "        gae = delta + gamma * lam * masks[step] *gae\n",
    "        returns.insert(0,gae + values[step])\n",
    "    return returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(env, model, device, deterministic=True,render=False):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.mean.detach().cpu().numpy()[0] if deterministic \\\n",
    "            else dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    # generates random mini-batches until we have covered the full batch\n",
    "    for _ in range(batch_size // mini_batch):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(states,actions,log_probs,returns,advantages,clip=epsilon):\n",
    "    for ep in range(ppo_epochs):\n",
    "        for state,action,old_log_prob,return_,advantage in ppo_iter(states,actions,log_probs,returns,advantages):\n",
    "            dist,value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_prob = dist.log_prob(action)\n",
    "            \n",
    "            ratio = (new_log_prob-old_log_prob).exp()\n",
    "            surr1 = ratio*advantage\n",
    "            surr2 = torch.clamp(ratio,1.0-clip,1.0+clip)*advantage\n",
    "            \n",
    "            actor_loss = -torch.min(surr1,surr2).mean()\n",
    "            critic_loss = (return_-value).pow(2).mean()\n",
    "            \n",
    "            loss = 0.5*critic_loss + actor_loss - entropy_beta*entropy\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PPO Algorithm\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [make_env() for i in range(n_env)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "env = gym.make(env_name)\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.shape[0]\n",
    "model = ActorCritic(num_inputs,num_outputs)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(1,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Reward :  -819.9795414228995\n",
      "Test Reward :  -709.0214086320943\n",
      "Test Reward :  -651.2910306227229\n",
      "Test Reward :  -480.77976063766135\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "state = envs.reset()\n",
    "best = -200\n",
    "for ep in range(n_episodes):\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    \n",
    "    for _ in range(PPO_STEPS):\n",
    "        state = torch.FloatTensor(state)\n",
    "        dist,value = model(state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        next_state,reward,done,_ = envs.step(action.numpy())\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1))\n",
    "        masks.append(torch.FloatTensor(1-done).unsqueeze(1))\n",
    "        states.append(state)  \n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    next_state = torch.FloatTensor(next_state)\n",
    "    _,next_value = model(next_state)\n",
    "    returns = compute_gae(next_value,rewards,masks,values)\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    advantage = normalize(advantage)\n",
    "    ppo_update(states,actions,log_probs,returns,advantage)\n",
    "\n",
    "    if (ep+1)%50 == 0:\n",
    "        test_reward = np.mean([test_env(env,model,'cpu') for _ in range(10)])\n",
    "        if test_reward > best:\n",
    "            print(\"Best Reward updated \",best,\" =====> \",test_reward)\n",
    "            best = test_reward\n",
    "            name = 'checkpoint_{}.pt'.format(best)\n",
    "            torch.save(model.state_dict(),name)\n",
    "        print(\"Test Reward : \",test_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadind the saved Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict = torch.load('checkpoint_-97.40409026063374.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watching the agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "deterministic = False\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    env.render()\n",
    "    dist, _ = model(state)\n",
    "    action = dist.mean.detach().cpu().numpy()[0] if deterministic \\\n",
    "        else dist.sample().cpu().numpy()[0]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reward = np.mean([test_env(env,model,'cpu',render=False) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
