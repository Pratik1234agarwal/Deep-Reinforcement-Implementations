{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='Reacher_Windows_x86_64/Reacher.exe')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30834824, -0.02160605,  0.34518294, -1.61076812]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)       # initialize the score (for each agent)\n",
    "for i in range(200):\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,input_size,output_size,std=0.00001):\n",
    "        super().__init__()\n",
    "        self.critic_1 = nn.Linear(input_size,32)\n",
    "        self.critic_2 = nn.Linear(32,1)\n",
    "        \n",
    "        self.mean_1 = nn.Linear(input_size,256)\n",
    "        self.mean_2 = nn.Linear(256,64)\n",
    "        self.mean_3 = nn.Linear(64,output_size)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(1, output_size) * std)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        \n",
    "        mean = F.relu(self.mean_1(state))\n",
    "        mean = F.relu(self.mean_2(mean))\n",
    "        mean = F.tanh(self.mean_3(mean))\n",
    "\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        dist = torch.distributions.Normal(mean,std)\n",
    "        \n",
    "        critic = F.relu(self.critic_1(state))\n",
    "        critic = self.critic_2(critic)\n",
    "        \n",
    "        return dist,critic\n",
    "    def critic(self,state):\n",
    "        \n",
    "        state = torch.from_numpy(state).float()\n",
    "        critic = F.relu(self.critic_1(state))\n",
    "        critic = self.critic_2(critic)\n",
    "        return critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,input_size,output_size,std=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(nn.Linear(input_size,256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(256,128) ,\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(64,output_size),\n",
    "                                  nn.Tanh())\n",
    "        self.critic_ = nn.Sequential(nn.Linear(input_size,32),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(32,16),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(16,1))\n",
    "        self.log_std = nn.Parameter(torch.ones(1,output_size)*std)\n",
    "    def forward(self,state):\n",
    "        value = self.critic_(state)\n",
    "        mu = self.actor(state)\n",
    "        std = self.log_std.exp()\n",
    "        dist = Normal(mu,std)\n",
    "        return dist,value\n",
    "    def critic(self,state):\n",
    "        return self.critic_(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(state_size,action_size)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "n_episodes = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_per_ep=[]\n",
    "for i in range(n_episodes):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "    rewards = 0\n",
    "    for j in range(1000):\n",
    "        state = torch.FloatTensor(state)\n",
    "        dist,critic = model(state)\n",
    "        action = dist.sample()\n",
    "        action = action.squeeze(0)\n",
    "        env_info = env.step(action.numpy())[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        reward = env_info.rewards\n",
    "        rewards += reward[0]\n",
    "        done = env_info.local_done\n",
    "        v_next = model.critic(next_state)\n",
    "        critic = critic.squeeze(0)\n",
    "        \n",
    "        advantage = (torch.tensor(reward)+gamma*v_next-critic)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #td.backward()\n",
    "        \n",
    "        critic_error = (advantage.pow(2)*0.5).mean()\n",
    "        actor_error = -(dist.log_prob(action) * advantage).mean()\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        loss = critic_error + actor_error - 0.0001 * entropy\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "        if done[0] or j==999:\n",
    "            reward_per_ep.append(rewards)\n",
    "            print(\"\\rEpisode {} , Avg reward : {:3f} , Score : {:3f} \".format(i+1,np.mean(reward_per_ep),rewards),end=\"\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(reward_per_ep)),reward_per_ep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PPO \n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,input_size,output_size,std=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(nn.Linear(input_size,256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(256,128) ,\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(128,output_size),\n",
    "                                  nn.Tanh())\n",
    "        self.critic = nn.Sequential(nn.Linear(input_size,32),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(32,16),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(16,1))\n",
    "        self.log_std = nn.Parameter(torch.ones(1,output_size)*std)\n",
    "    def forward(self,state):\n",
    "        value = self.critic(state)\n",
    "        mu = self.actor(state)\n",
    "        std = self.log_std.exp()\n",
    "        dist = Normal(mu,std)\n",
    "        return dist,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_env = 8 \n",
    "gamma = 0.99\n",
    "lr = 1e-6\n",
    "gae_lambda = 0.95\n",
    "epsilon = 0.1\n",
    "entropy_beta = 0.001\n",
    "critic_discount = 0.5\n",
    "PPO_STEPS = 1000\n",
    "mini_batch = 32\n",
    "ppo_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value,rewards,masks,values,gamma=gamma,lam=gae_lambda):\n",
    "    values = values+[next_value]\n",
    "    gae = 0 \n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma*values[step+1]*masks[step] - values[step]\n",
    "        gae = delta + gamma * lam * masks[step] *gae\n",
    "        returns.insert(0,gae + values[step])\n",
    "    return returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(train_mode=False):\n",
    "    env_info = env.reset(train_mode)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    \n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    # generates random mini-batches until we have covered the full batch\n",
    "    for _ in range(batch_size // mini_batch):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(states,actions,log_probs,returns,advantages,clip=epsilon):\n",
    "    for ep in range(ppo_epochs):\n",
    "        for state,action,old_log_prob,return_,advantage in ppo_iter(states,actions,log_probs,returns,advantages):\n",
    "            dist,value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_prob = dist.log_prob(action)\n",
    "            \n",
    "            ratio = (new_log_prob-old_log_prob).exp()\n",
    "        \n",
    "            surr1 = ratio*advantage\n",
    "            surr2 = torch.clamp(ratio,1.0-clip,1.0+clip)*advantage\n",
    "            \n",
    "            actor_loss = -torch.min(surr1,surr2).mean()\n",
    "            critic_loss = (return_-value).pow(2).mean()\n",
    "            \n",
    "            \n",
    "            loss = 0.5*critic_loss + actor_loss - entropy_beta*entropy\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(state_size,action_size)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist,value = model(torch.FloatTensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.6779608 , -0.34429672, -0.06391853]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip(dist.sample().numpy(),-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 58 , Score : 0.000000"
     ]
    }
   ],
   "source": [
    "n_env = 8 \n",
    "gamma = 0.99\n",
    "lr = 1e-4\n",
    "epsilon = 0.12\n",
    "entropy_beta = 0.0001\n",
    "critic_discount = 0.5\n",
    "PPO_STEPS = 512\n",
    "mini_batch = 64\n",
    "ppo_epochs = 10\n",
    "\n",
    "def compute_gae(next_value,rewards,masks,values,gamma=gamma,lam=gae_lambda):\n",
    "    values = values+[next_value]\n",
    "    gae = 0 \n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma*values[step+1]*masks[step] - values[step]\n",
    "        gae = delta + gamma * lam * masks[step] *gae\n",
    "        returns.insert(0,gae + values[step])\n",
    "    return returns \n",
    "\n",
    "def normalize(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-8)\n",
    "    return x\n",
    "\n",
    "def test_env(train_mode=False):\n",
    "    env_info = env.reset(train_mode)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    \n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "\n",
    "def ppo_iter(states, actions, log_probs,advantage):\n",
    "    batch_size = states.size(0)\n",
    "    # generates random mini-batches until we have covered the full batch\n",
    "    for _ in range(batch_size // mini_batch):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :],advantage[rand_ids, :]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def ppo_update(states,actions,log_probs,advantages,clip=epsilon):\n",
    "    for ep in range(ppo_epochs):\n",
    "        for state,action,old_log_prob,advantage in ppo_iter(states,actions,log_probs,advantages):\n",
    "            dist,value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_prob = dist.log_prob(action)\n",
    "            \n",
    "            ratio = (new_log_prob-old_log_prob).exp()\n",
    "        \n",
    "            surr1 = ratio*advantage\n",
    "            surr2 = torch.clamp(ratio,1.0-clip,1.0+clip)*advantage\n",
    "            \n",
    "            actor_loss = -torch.min(surr1,surr2).mean()\n",
    "            #critic_loss = (return_-value).pow(2).mean()\n",
    "            critic_loss = advantage.pow(2).mean()\n",
    "            \n",
    "            loss = 0.5*critic_loss + actor_loss - entropy_beta*entropy\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            return actor_loss.item()\n",
    "\n",
    "model = ActorCritic(state_size,action_size)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations\n",
    "\n",
    "dist,value = model(torch.FloatTensor(state))\n",
    "\n",
    "np.clip(dist.sample().numpy(),-1,1)\n",
    "ac_loss = []\n",
    "n_episodes = 100\n",
    "best = 5\n",
    "reward_per_ep = []\n",
    "for ep in range(n_episodes):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    score = 0\n",
    "    c=0\n",
    "    dones = []\n",
    "    while True:\n",
    "        state = torch.FloatTensor(state)\n",
    "        dist,value = model(state)\n",
    "        action = dist.sample()\n",
    "        env_info = env.step(np.clip(action.numpy(),-1,1))[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done[0]\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1))\n",
    "        #masks.append(torch.FloatTensor(1-done).unsqueeze(1))\n",
    "        states.append(state)  \n",
    "        actions.append(action)\n",
    "        dones.append(torch.FloatTensor(1-done))\n",
    "        state = next_state\n",
    "        score += reward[0]\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    reward_per_ep.append(score)\n",
    "    next_state = torch.FloatTensor(next_state)\n",
    "    _,next_value = model(next_state)\n",
    "    #returns = compute_gae(next_value,rewards,masks,values)    Testing\n",
    "    # returns   = torch.cat(returns).detach()             Testing\n",
    "    rewards   = torch.cat(rewards)\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    \n",
    "    ###Testing Code\n",
    "    advantage = torch.zeros(1001)\n",
    "    for i in range(len(rewards)-1):\n",
    "        temp = rewards[i][0] + gamma*values[i+1][0] - values[i][0]\n",
    "        advantage[i] = torch.FloatTensor([temp])\n",
    "    \n",
    "    advantage[1000] = rewards[1000][0] - values[1000][0]\n",
    "    advantage = advantage.view(1001,1)\n",
    "    #######\n",
    "    \n",
    "    \n",
    "    #advantage = returns - values\n",
    "    #advantage = normalize(advantage)\n",
    "    loss = ppo_update(states,actions,log_probs,advantage)\n",
    "    ac_loss.append(loss)\n",
    "    \n",
    "    print(\"\\rEpisode {} , Score : {:3f}\".format(ep+1,score),end=\"\")\n",
    "    if ep > 100 :\n",
    "        print(\" Avg of last 100 : {:2f} \".format(np.mean(reward_per_ep[-100:])),end=\"\")\n",
    "    if score> best:\n",
    "        print(\"Best Reward updated \",best,\" =====> \",test_reward)\n",
    "        best = score\n",
    "        name = 'checkpoint_{}.pt'.format(best)\n",
    "        torch.save(model.state_dict(),name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(reward_per_ep))+1,reward_per_ep,label='Score')\n",
    "plt.plot(np.arange(len(ac_loss))+1,ac_loss,label='Actor Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
