{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization in Discrete Action Space\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor_ = nn.Sequential(nn.Linear(input_size,32),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(32,16),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(16,output_size),\n",
    "                                  nn.Softmax(dim=1))\n",
    "        self.critic_ = nn.Sequential(nn.Linear(input_size,32),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(32,16),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(16,1))\n",
    "    def forward(self,state):\n",
    "        actor = self.actor_(state)\n",
    "        dist = torch.distributions.Categorical(actor)\n",
    "        return dist,self.critic_(state)\n",
    "    def critic(self,state):\n",
    "        return self.critic_(state)\n",
    "    def actor(self,state):\n",
    "        actor = self.actor_(state)\n",
    "        dist = torch.distributions.Categorical(actor)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Hyperparameters List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "entropy_beta = 0.001\n",
    "iterations = 1500\n",
    "target_reward = -80\n",
    "env_name = 'CartPole-v0'\n",
    "critic_discount = 0.5\n",
    "lr = 0.0001\n",
    "mini_batch = 64\n",
    "ppo_steps = 256\n",
    "ppo_epochs = 10\n",
    "epsilon=0.2\n",
    "n_envs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions (like computing GAE and PPO update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value,values,rewards,mask,gamma=gamma,lamda=gae_lambda):\n",
    "    values = values + [next_value]\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + masks[step]*gamma*values[step+1] - values[step]\n",
    "        gae = delta + gamma*lamda*masks[step]*gae\n",
    "        returns.insert(0,gae+values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    m = x.mean()\n",
    "    std = x.std()\n",
    "    x-=m\n",
    "    x/=(std+1e-9)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(states,action,log_probs,returns,advantage):\n",
    "    batch_size = states.size(0)\n",
    "    \n",
    "    for _ in range(batch_size//mini_batch):\n",
    "        idxs = np.random.randint(0,batch_size,mini_batch)\n",
    "        s = states[idxs,:]\n",
    "        a = action[idxs,:]\n",
    "        l = log_probs[idxs,:]\n",
    "        r = returns[idxs,:]\n",
    "        a = advantage[idxs,:]\n",
    "        yield states[idxs,:],action[idxs,:],log_probs[idxs,:],returns[idxs,:],advantage[idxs,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(states,actions,log_probs,returns,advantages,clip=epsilon):\n",
    "    for ep in range(ppo_epochs):\n",
    "        for state,action,old_log_prob,return_,advantage in ppo_iter(states,actions,log_probs,returns,advantages):\n",
    "            dist,value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_prob = dist.log_prob(action)\n",
    "            \n",
    "            ratio = (new_log_prob-old_log_prob).exp()\n",
    "            surr1 = ratio*advantage\n",
    "            surr2 = torch.clamp(ratio,1.0-clip,1.0+clip)*advantage\n",
    "            \n",
    "            actor_loss = -torch.min(surr1,surr2).mean()\n",
    "            critic_loss = critic_discount*(return_- value).pow(2).mean()\n",
    "            \n",
    "            loss = actor_loss + critic_loss - entropy_beta * entropy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(env,model,device,render=False):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist = model.actor(state)\n",
    "        action = dist.sample().detach().cpu().numpy()[0]\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        score+=reward\n",
    "        if done:\n",
    "            break\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining the Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    def _thunk():\n",
    "        return gym.make(env_name)\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State info :  Box(4,)\n",
      "Action info :  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "envs = [make_env() for _ in range(n_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "env = gym.make(env_name)\n",
    "n_input = env.observation_space.shape[0]\n",
    "n_output = env.action_space.n\n",
    "print(\"State info : \",env.observation_space)\n",
    "print(\"Action info : \",env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initializing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(n_input,n_output)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Puting it all together ( training )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50 , Score = 19.35\n",
      "Best Reward updated :  -250  ====>  19.35\n",
      "Iteration 100 , Score = 25.35\n",
      "Best Reward updated :  19.35  ====>  25.35\n",
      "Iteration 150 , Score = 23.05\n",
      "Iteration 200 , Score = 22.9\n",
      "Iteration 250 , Score = 25.0\n",
      "Iteration 300 , Score = 22.1\n",
      "Iteration 350 , Score = 24.55\n",
      "Iteration 400 , Score = 24.05\n",
      "Iteration 450 , Score = 20.75\n",
      "Iteration 500 , Score = 21.6\n",
      "Iteration 550 , Score = 26.2\n",
      "Best Reward updated :  25.35  ====>  26.2\n",
      "Iteration 600 , Score = 19.35\n",
      "Iteration 650 , Score = 21.95\n",
      "Iteration 700 , Score = 23.65\n",
      "Iteration 750 , Score = 18.45\n",
      "Iteration 800 , Score = 21.6\n",
      "Iteration 850 , Score = 23.7\n",
      "Iteration 900 , Score = 21.75\n",
      "Iteration 950 , Score = 23.45\n",
      "Iteration 1000 , Score = 20.6\n",
      "Iteration 1050 , Score = 20.2\n"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "best = -250\n",
    "for it in range(iterations):\n",
    "    log_probs = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    states = []\n",
    "    masks = []\n",
    "    rewards = []\n",
    "    \n",
    "    for steps in range(ppo_steps):\n",
    "        state = torch.FloatTensor(state)\n",
    "        dist,value = model(state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        \n",
    "        next_state,reward,done,_ = envs.step(action.numpy())\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        log_probs.append(log_prob.view(8,1))\n",
    "        states.append(state)\n",
    "        values.append(value.view(8,1))\n",
    "        actions.append(action.view(8,1))\n",
    "        masks.append(torch.FloatTensor(1-done).unsqueeze(1))\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1))\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    next_state = torch.FloatTensor(next_state)\n",
    "    next_value = model.critic(next_state)\n",
    "    \n",
    "    returns = compute_gae(next_value,values,rewards,masks)\n",
    "    returns = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values = torch.cat(values).detach()\n",
    "    states = torch.cat(states).detach()\n",
    "    actions = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    advantage = normalize(advantage)\n",
    "    \n",
    "    ppo_update(states,actions,log_probs,returns,advantage)\n",
    "    \n",
    "    \n",
    "    if (it+1) % 50 == 0:\n",
    "        test_reward = np.mean([test_env(env,model,'cpu') for _ in range(20)])\n",
    "        print(\"Iteration {} , Score = {}\".format(it+1,test_reward))\n",
    "        if test_reward > best :\n",
    "            print(\"Best Reward updated : \",best,\" ====> \",test_reward)\n",
    "            best = test_reward\n",
    "            name = 'checkpoint_acro_{}.pt'.format(round(best))\n",
    "            torch.save(model.state_dict(),name)\n",
    "\n",
    "        if best > 198 :\n",
    "            print(\"====================== Enviorement Solved ====================\")\n",
    "            torch.save(model.state_dict(),'best_acro.pt')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loading the Best Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'chekpoint_latest.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-ccdee691883d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'chekpoint_latest.pt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chekpoint_latest.pt'"
     ]
    }
   ],
   "source": [
    "name = 'chekpoint_latest.pt'\n",
    "model.state_dict = torch.load(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Watching the trained AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.mean([test_env(env,model,'cpu',render=True) for _ in range(1)])\n",
    "print(\"Avg. of 100 Games {:3f}\".format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
